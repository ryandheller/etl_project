{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession, user_agent\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#access token for api\n",
    "api_key = \"REVeQu6ErJR69xuO8n21kyuUA0Xt4lV1\"\n",
    "# first published date online\n",
    "starts_date = \"02/13/11\"\n",
    "#convert to Y-M-D\n",
    "start_date = datetime.strptime(starts_date, \"%m/%d/%y\")\n",
    "# create a list of list publication dates\n",
    "date_list=[]\n",
    "while start_date <= datetime.today():\n",
    "    date_list.append(start_date.strftime('%Y-%m-%d'))\n",
    "    start_date = (start_date + timedelta(days=7))\n",
    "# take dates to create a list of api urls for all dates\n",
    "def make_url(date):\n",
    "    url = f\"https://api.nytimes.com/svc/books/v3/lists/{date}/combined-print-and-e-book-fiction.json?api-key={api_key}\"\n",
    "    return url\n",
    "api_urls = []\n",
    "for date in date_list:\n",
    "    api_url = make_url(date)\n",
    "    api_urls.append(api_url)\n",
    "\n",
    "# download the api files\n",
    "def download_api_files(url):\n",
    "    data = requests.get(url).json()\n",
    "    fullname = \"api_download\\ \"+ url.split('/')[7] + \".json\"\n",
    "    with open(fullname, 'w') as outfile:  \n",
    "        json.dump(data, outfile)\n",
    "\n",
    "#only download new api files\n",
    "api_list = [x for x in glob.glob('api_download/*.json')]\n",
    "downloaded_dates = []\n",
    "for api in api_list:\n",
    "    date = (api.split('\\ ')[1].split('.')[0])\n",
    "    downloaded_dates.append(date)\n",
    "\n",
    "for url in api_urls:\n",
    "    url_date = url.split('/')[7]\n",
    "    if url_date in downloaded_dates:\n",
    "        'alreaady downloaded'\n",
    "    else:\n",
    "        print(url_date)\n",
    "        download_api_files(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to download amazon page html\n",
    "def download_book_data_from_url(url):\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        r = session.get(url)\n",
    "        r.html.render()\n",
    "    except Exception as e:\n",
    "        print('books borked!', e)\n",
    "    soup = BeautifulSoup(r.html.html, \"html.parser\")\n",
    "    fullname = \"htmlFolder\\ \"+ url.split('/')[3] + \".html\"\n",
    "    print(fullname)\n",
    "    with open(fullname, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "    \n",
    "\n",
    "#gather the amazon urls from the best selling apis\n",
    "with open('ny_times_dict.pkl','rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "amazon_urls = []\n",
    "for a in b:\n",
    "    for z in a:\n",
    "        if z.get('amazon_url') != None:\n",
    "            amazon_urls.append(z.get('amazon_url'))\n",
    "        elif z.get('amazon_url') == None:\n",
    "            ''\n",
    "#by sending our list to a set we get rid of any repeats\n",
    "amazon_set = set(amazon_urls)\n",
    "print(len(amazon_urls), len(amazon_set))\n",
    "\n",
    "counting = 0\n",
    "# check to see if a book has already had it's html downloaded, if not download it\n",
    "for url in amazon_set:\n",
    "    new_u = url.split('/')[3]\n",
    "    existing_files = [x.split('\\\\')[-1].strip() for x in glob.glob('htmlFolder/*.html')]\n",
    "    existing_names = [x.split('.')[0].strip() for x in existing_files]\n",
    "    if new_u == 'The-Lady-Rivers-Novel-Cousins?tag=NYTBS-20':\n",
    "        print('get outta here', url)\n",
    "    elif new_u not in existing_names:\n",
    "            download_book_data_from_url(url)\n",
    "            print('downloading', url)\n",
    "    elif new_u in existing_names:\n",
    "        'nothing'\n",
    "        print('already downloaded')\n",
    "    #counting is just to watch where the file is at when running\n",
    "    counting += 1\n",
    "    print(counting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "#grabbing the downloaded api files\n",
    "api_list = [x for x in glob.glob('api_download/*.json')]\n",
    "#creating list of dictionaries to be pickled later\n",
    "dicts_list =[]\n",
    "# gathering information for each book on the best sellers list, sending to dicts_list\n",
    "for api in api_list:\n",
    "    with open(api) as f:\n",
    "        data = json.load(f)\n",
    "        dicts = []\n",
    "        date = data.get('results').get('published_date')\n",
    "        books = data.get('results').get('books')[:15]\n",
    "        for book in books:\n",
    "            rank = book.get('rank')\n",
    "            title = book.get('title')\n",
    "            author = book.get('author')\n",
    "            amazon_url = book.get('amazon_product_url')\n",
    "            book_dict={\"rank\":rank,\"title\":title, \"author\":author,\"amazon_url\":amazon_url,\"date\":date}\n",
    "            dicts.append(book_dict)\n",
    "    # count is just to check where the file is at while running\n",
    "    count += 1\n",
    "    print(count)\n",
    "    dicts_list.append(dicts)\n",
    "\n",
    "#pickle dicts_list for later use\n",
    "with open('ny_times_dict.pkl', 'wb') as g:\n",
    "    pickle.dump(dicts_list, g, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#check pickling\n",
    "with open('ny_times_dict.pkl','rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "print(len(dicts_list))\n",
    "print(dicts_list == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read html and gather the prices for each book in the htmlFolder\n",
    "price_dict = []\n",
    "count = 0\n",
    "def read_html_find_price(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    try:\n",
    "        prices = soup.find('ul', {\"class\": \"a-unordered-list a-nostyle a-button-list a-horizontal\"})\\\n",
    "        .find_all(\"span\", {\"class\": \"a-button-inner\"})\n",
    "    except Exception as e:\n",
    "        print('no soup for u', e, html_file) \n",
    "    results = {}\n",
    "    strip_name = html_file.split('\\\\')[-1].strip()\n",
    "    name = strip_name.split('.')[0].strip()\n",
    "    for p in prices:\n",
    "        if \"Kindle\" in p.get_text():\n",
    "            try:\n",
    "                result = re.findall('\\$\\d{1,}\\.\\d{1,}', p.get_text())\n",
    "                results['Kindle'] = float(result[0].strip('[').strip('$'))\n",
    "            except: ''\n",
    "        elif \"Hardcover\" in p.get_text():\n",
    "            try:\n",
    "                result = re.findall('\\$\\d{1,}\\.\\d{1,}', p.get_text())\n",
    "                results['Hardcover'] = float(result[0].strip('[').strip('$'))\n",
    "            except: ''\n",
    "        elif \"Paperback\" in p.get_text():\n",
    "            try:\n",
    "                result = re.findall('\\$\\d{1,}\\.\\d{1,}', p.get_text())\n",
    "                results['Paperback'] = float(result[0].strip('[').strip('$'))\n",
    "            except: ''\n",
    "        elif \"Audio\" in p.get_text(): \n",
    "            try:\n",
    "                result = re.findall('\\$\\d{1,}\\.\\d{1,}', p.get_text())\n",
    "                results['Audio'] = float(result[0].strip('[').strip('$'))\n",
    "            except: ''\n",
    "        results['Title'] = name\n",
    "    price_dict.append(results)\n",
    "    # count to keep track of where the function is at\n",
    "    \n",
    "# only run the function on valid HTML files (some amazon links no longer exist (less than 5%))\n",
    "\n",
    "with open('amazon_prices.pkl','rb') as handle:\n",
    "    amazon_prices_grabbed = pickle.load(handle)\n",
    "prices_grabbed = []\n",
    "for price in amazon_prices_grabbed :\n",
    "    price_dict.append(price)\n",
    "    prices_grabbed.append(price.get('Title')) \n",
    "\n",
    "existing_files = [x for x in glob.glob('htmlFolder/*.html')]\n",
    "for files in existing_files:\n",
    "    file_name = files.split('\\\\')[1].split('.')[0].strip(' ')\n",
    "    if os.path.getsize(files) > 2500:\n",
    "        if file_name in prices_grabbed:\n",
    "            print('good to go')\n",
    "        else:\n",
    "            try:\n",
    "                read_html_find_price(files)\n",
    "                count +=1\n",
    "                print(count)\n",
    "            except:''\n",
    "    else:\n",
    "        print('url is not good')\n",
    "\n",
    "print(len(price_dict))    \n",
    "# pickle the results\n",
    "with open('amazon_prices.pkl', 'wb') as f:\n",
    "    pickle.dump(price_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('amazon_prices.pkl','rb') as handle:\n",
    "#     b = pickle.load(handle)\n",
    "# print(price_dict == b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#open both pkl files\n",
    "with open('ny_times_dict.pkl','rb') as handle:\n",
    "    ny_times = pickle.load(handle)\n",
    "\n",
    "with open('amazon_prices.pkl','rb') as handle:\n",
    "    amazon = pickle.load(handle)\n",
    "#zip together dictionaries for all books that had valid amazon urls\n",
    "good_dicts = []\n",
    "for listed in ny_times:\n",
    "    for dicts in listed:\n",
    "        try:\n",
    "            title = dicts.get('amazon_url').split('/')[3]\n",
    "        except:\n",
    "            'oh whale'\n",
    "        for item in amazon:\n",
    "            if item.get('Title') == title:\n",
    "                z = {**item, **dicts}\n",
    "                good_dicts.append(z)\n",
    "                \n",
    "            else:\n",
    "                ''\n",
    "#create a list of amazon urls added to good_dicts\n",
    "amazon_urls = []\n",
    "for dicts in good_dicts:\n",
    "    amazon_urls.append(dicts.get('amazon_url'))\n",
    "#check and add any remaining books to good_dicts\n",
    "for listed in ny_times:\n",
    "    for dicts in listed:\n",
    "        if dicts.get('amazon_url') in amazon_urls:\n",
    "            'good to go'\n",
    "        else:\n",
    "            good_dicts.append(dicts)\n",
    "#for those books that didn't return amazon prices append a blank\n",
    "kindle = []\n",
    "for dicts in good_dicts:\n",
    "    if 'Kindle' in dicts.keys():\n",
    "        kindle.append(dicts['Kindle'])\n",
    "    else:\n",
    "        dicts['Kindle']=float(0)\n",
    "        kindle.append(dicts['Kindle'])\n",
    "    if 'Hardcover' in dicts.keys():\n",
    "        kindle.append(dicts['Hardcover'])\n",
    "    else:\n",
    "        dicts['Hardcover']=float(0)\n",
    "        kindle.append(dicts['Kindle'])\n",
    "    if 'Paperback' in dicts.keys():\n",
    "        kindle.append(dicts['Paperback'])\n",
    "    else:\n",
    "        dicts['Paperback']=float(0)\n",
    "        kindle.append(dicts['Kindle'])\n",
    "    if 'Audio' in dicts.keys():\n",
    "        kindle.append(dicts['Kindle'])\n",
    "    else:\n",
    "        dicts['Audio']= float(0)\n",
    "        kindle.append(dicts['Kindle'])\n",
    "\n",
    "\n",
    "# send dictionary to MongoDB\n",
    "client = MongoClient()\n",
    "db = client.NYT_best_sellers\n",
    "db.info.remove()\n",
    "db.info.insert(good_dicts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
